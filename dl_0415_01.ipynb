{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20059541.383098383"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f1(x):\n",
    "    return x**2 + 2*x +3\n",
    "\n",
    "\n",
    "def diff1(x):\n",
    "    return 2*x+2\n",
    "\n",
    "\n",
    "X = np.random.randn(100,20)\n",
    "y = np.random.randn(100,1)\n",
    "\n",
    "W1 = np.random.randn(20,100)\n",
    "W2 = np.random.randn(100,1000)\n",
    "W3 = np.random.randn(1000,10)\n",
    "W4 = np.random.randn(10,1)\n",
    "y_hat = np.dot(np.dot(np.dot(np.dot(X,W1),W2),W3),W4)\n",
    "\n",
    "loss = np.mean((y_hat -y)**2)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.layer = []\n",
    "    \n",
    "    def add(self, layer):        \n",
    "        if len(self.layer) == 0:\n",
    "            self.layer.append(np.random.randn(self.x.shape[1], layer))\n",
    "        else:\n",
    "            self.layer.append(np.random.randn(self.layer[-1].shape[1], layer))\n",
    "    \n",
    "    def predict(self):\n",
    "        y_hat = self.x.copy()\n",
    "\n",
    "        for layer in self.layer:\n",
    "            y_hat = np.dot(y_hat, layer)\n",
    "        \n",
    "        self.y_hat = y_hat\n",
    "        return self.y_hat\n",
    "    \n",
    "    def loss(self):\n",
    "        self.loss_ = np.mean((self.predict() - self.y)**2)\n",
    "        return self.loss_\n",
    "\n",
    "    def grad(self, w):\n",
    "        f = self.loss\n",
    "        h = 1e-4\n",
    "        self.grad_ = np.zeros_like(w)\n",
    "        for i in range(w.shape[0]):\n",
    "            for j in range(w.shape[1]):\n",
    "                tmp = w[i,j]\n",
    "                fx = f(w)\n",
    "                w[i,j] = tmp + h\n",
    "                fxh = f(w)\n",
    "                w[i,j] = tmp\n",
    "                self.grad_[i,j] = (fxh - fx)/h\n",
    "        return  self.grad_\n",
    "    \n",
    "    def gradient_descent(self):\n",
    "        for i, w in enumerate(self.layer):\n",
    "            self.layer[i] -= 1e-3 * self.grad(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Net.loss() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m net\u001b[39m.\u001b[39madd(\u001b[39m50\u001b[39m)\n\u001b[1;32m      6\u001b[0m net\u001b[39m.\u001b[39madd(\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m net\u001b[39m.\u001b[39;49mgradient_descent()\n",
      "Cell \u001b[0;32mIn[34], line 42\u001b[0m, in \u001b[0;36mNet.gradient_descent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgradient_descent\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     41\u001b[0m     \u001b[39mfor\u001b[39;00m i, w \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer):\n\u001b[0;32m---> 42\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer[i] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1e-3\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrad(w)\n",
      "Cell \u001b[0;32mIn[34], line 33\u001b[0m, in \u001b[0;36mNet.grad\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(w\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n\u001b[1;32m     32\u001b[0m     tmp \u001b[39m=\u001b[39m w[i,j]\n\u001b[0;32m---> 33\u001b[0m     fx \u001b[39m=\u001b[39m f(w)\n\u001b[1;32m     34\u001b[0m     w[i,j] \u001b[39m=\u001b[39m tmp \u001b[39m+\u001b[39m h\n\u001b[1;32m     35\u001b[0m     fxh \u001b[39m=\u001b[39m f(w)\n",
      "\u001b[0;31mTypeError\u001b[0m: Net.loss() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "net = Net(X, y)\n",
    "net.add(100)\n",
    "net.add(1000)\n",
    "net.add(100)\n",
    "net.add(50)\n",
    "net.add(1)\n",
    "\n",
    "net.gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(f, w):\n",
    "    h = 1e-4\n",
    "    g = np.zeros_like(w)\n",
    "    for i in range(w.shape[0]):\n",
    "        for j in range(w.shape[1]):\n",
    "            tmp = w[i,j]\n",
    "            fx = f(w)\n",
    "            w[i,j] = tmp + h\n",
    "            fxh = f(w)\n",
    "            w[i,j] = tmp\n",
    "            g[i,j] = (fxh - fx)/h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Net.loss() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m net\u001b[39m.\u001b[39madd(\u001b[39m50\u001b[39m)\n\u001b[1;32m      6\u001b[0m net\u001b[39m.\u001b[39madd(\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m net\u001b[39m.\u001b[39;49mgradient_descent()\n",
      "Cell \u001b[0;32mIn[29], line 42\u001b[0m, in \u001b[0;36mNet.gradient_descent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgradient_descent\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     41\u001b[0m     \u001b[39mfor\u001b[39;00m i, w \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer):\n\u001b[0;32m---> 42\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer[i] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1e-3\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrad(w)\n",
      "Cell \u001b[0;32mIn[29], line 33\u001b[0m, in \u001b[0;36mNet.grad\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(w\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n\u001b[1;32m     32\u001b[0m     tmp \u001b[39m=\u001b[39m w[i,j]\n\u001b[0;32m---> 33\u001b[0m     fx \u001b[39m=\u001b[39m f(w)\n\u001b[1;32m     34\u001b[0m     w[i,j] \u001b[39m=\u001b[39m tmp \u001b[39m+\u001b[39m h\n\u001b[1;32m     35\u001b[0m     fxh \u001b[39m=\u001b[39m f(w)\n",
      "\u001b[0;31mTypeError\u001b[0m: Net.loss() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "net = Net(X, y)\n",
    "net.add(100)\n",
    "net.add(1000)\n",
    "net.add(100)\n",
    "net.add(50)\n",
    "net.add(1)\n",
    "\n",
    "net.gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 100)\n",
      "(100, 1000)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(net.layers)):\n",
    "    print(net.layer[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_differentiation(f, x, method='central'):\n",
    "  \"\"\"\n",
    "  첫번째 인자 f는 미분하려고 하는 함수\n",
    "  두번째 인자 x는 x의 값 \n",
    "  세번째 인자 method = 'central'  중앙차분이 디폴트.\n",
    "                     'forward' 전방차분\n",
    "                     'backward' 후방차분\n",
    "  \"\"\"\n",
    "  delta_x = 1e-4  # 극한값을 고려. 델타x는 0에 최대한 가까워야함. 여기선 1 * 10^(-4)를 사용\n",
    "  \n",
    "  if method == 'central':\n",
    "    result = (f(x + delta_x) - f(x - delta_x)) / (2 * delta_x)\n",
    "  elif method == 'forward':\n",
    "    result = (f(x + delta_x) - f(x)) / delta_x\n",
    "  elif method == 'backward':\n",
    "    result = (f(x) - f(x - delta_x)) / delta_x\n",
    "  else:\n",
    "    raise ValueError(\"Method must be either 'central', 'forward', or 'backward'\")\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.51496882, -1.55592299,  0.53191366, ..., -0.82695777,\n",
       "         0.09989883, -1.26061498],\n",
       "       [-0.67874158,  0.11627723,  0.21976401, ..., -0.70588668,\n",
       "        -0.64488293,  0.14876006],\n",
       "       [ 1.84878109,  0.77822767,  0.37307817, ..., -2.72480833,\n",
       "         1.3474134 ,  0.26446687],\n",
       "       ...,\n",
       "       [-0.25409022, -1.15368596,  0.77946569, ...,  0.26458358,\n",
       "        -2.03909152, -2.52004513],\n",
       "       [ 0.87005259, -2.90329733,  0.74771956, ..., -0.9124526 ,\n",
       "         0.40035809, -0.17630215],\n",
       "       [ 0.8535451 ,  1.2836396 ,  0.4906499 , ...,  1.06622522,\n",
       "         0.21457739, -1.04817755]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net:\n",
    "    def __init__(self):\n",
    "        self.layer = []\n",
    "    \n",
    "    def add(self, layer):        \n",
    "        if len(self.layer) == 0:\n",
    "            self.layer.append(np.random.randn(100, layer))\n",
    "        else:\n",
    "            self.layer.append(np.random.randn(self.layer[-1].shape[1], layer))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        y_hat = x\n",
    "        self.layer[0] = np.random.randn(x.shape[0], self.layer[0].shape[1])\n",
    "        for layer in self.layer:\n",
    "            y_hat = np.dot(y_hat, layer)\n",
    "        \n",
    "        self.y_hat = y_hat\n",
    "        return self.y_hat\n",
    "    \n",
    "    def loss(self, y):\n",
    "        self.loss_ = np.mean((self.predict(x) - y)**2)\n",
    "        return self.loss_\n",
    "\n",
    "    def grad(self, w):\n",
    "        f = self.loss\n",
    "        h = 1e-4\n",
    "        self.grad_ = np.zeros_like(w)\n",
    "        for i in range(w.shape[0]):\n",
    "            for j in range(w.shape[1]):\n",
    "                tmp = w[i,j]\n",
    "                fx = f(w)\n",
    "                w[i,j] = tmp + h\n",
    "                fxh = f(w)\n",
    "                w[i,j] = tmp\n",
    "                self.grad_[i,j] = (fxh - fx)/h\n",
    "        return  self.grad_\n",
    "    \n",
    "    def gradient_descent(self):\n",
    "        for i, w in enumerate(self.layer):\n",
    "            self.layer[i] -= 1e-3 * self.grad(w)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
